{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "from utils import path_detection_utils as path_util\n",
    "from utils import data_collection_util as data_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading in model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "NUM_CLASSES = 90\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection on specific frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = 'video_frames/frame199.jpg'\n",
    "path2 = 'video_frames/frame200.jpg'\n",
    "path3 = 'video_frames/frame201.jpg'\n",
    "\n",
    "path4 = 'test_images/pic.jpg'\n",
    "\n",
    "image1 = Image.open(path1)\n",
    "width1, height1 = image1.size\n",
    "image_np1 = path_util.load_image_into_numpy_array(image1)\n",
    "boxes1, scores1, classes1 = path_util.get_segmentation(path1,detection_graph, category_index, False)\n",
    "human_boxes1, human_scores1 = path_util.remove_non_human(boxes1, scores1, classes1)\n",
    "good_boxes1, good_scores1 = path_util.remove_low_prob(human_boxes1, human_scores1)\n",
    "good_boxes1, good_scores1 = path_util.remove_poorly_sized_people(good_boxes1, good_scores1)\n",
    "\n",
    "image2 = Image.open(path2)\n",
    "width2, height2 = image2.size\n",
    "image_np2 = path_util.load_image_into_numpy_array(image2)\n",
    "boxes2, scores2, classes2 = path_util.get_segmentation(path2,detection_graph, category_index, False)\n",
    "human_boxes2, human_scores2 = path_util.remove_non_human(boxes2, scores2, classes2)\n",
    "good_boxes2, good_scores2 = path_util.remove_low_prob(human_boxes2, human_scores2)\n",
    "good_boxes2, good_scores2 = path_util.remove_poorly_sized_people(good_boxes2, good_scores2)\n",
    "\n",
    "image3 = Image.open(path3)\n",
    "width3, height3 = image3.size\n",
    "image_np3 = path_util.load_image_into_numpy_array(image3)\n",
    "boxes3, scores3, classes3 = path_util.get_segmentation(path3,detection_graph, category_index, False)\n",
    "human_boxes3, human_scores3 = path_util.remove_non_human(boxes3, scores3, classes3)\n",
    "good_boxes3, good_scores3 = path_util.remove_low_prob(human_boxes3, human_scores3)\n",
    "good_boxes3, good_scores3 = path_util.remove_poorly_sized_people(good_boxes3, good_scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_frames/frame199.jpg\n",
      "video_frames/frame200.jpg\n",
      "(720, 1280)\n",
      "(720, 1280)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/Users/travis/build/skvark/opencv-python/opencv/modules/video/src/optflowgf.cpp:1114: error: (-215) prev0.size() == next0.size() && prev0.channels() == next0.channels() && prev0.channels() == 1 && pyrScale_ < 1 in function calc\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-338371571bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video_frames/frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m199\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/abigailshchur/Desktop/Path-Detection/object_detection/utils/data_collection_util.py\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(path, start_frame, end_frame, detection_graph, category_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mnext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_frame\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m# flow should contain optical flow matrix?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optical_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# running image segmentation on current frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/abigailshchur/Desktop/Path-Detection/object_detection/utils/data_collection_util.py\u001b[0m in \u001b[0;36mget_optical_flow\u001b[0;34m(path1, path2)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m#flow = cv2.calcOpticalFlowFarneback(prvs,nxt,None, 0.5,3.0,15.0,3.0,5.0,1.2,0.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalcOpticalFlowFarneback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprvs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m#mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/travis/build/skvark/opencv-python/opencv/modules/video/src/optflowgf.cpp:1114: error: (-215) prev0.size() == next0.size() && prev0.channels() == next0.channels() && prev0.channels() == 1 && pyrScale_ < 1 in function calc\n"
     ]
    }
   ],
   "source": [
    "stuff = data_util.collect_data('video_frames/frame', 199, 202, detection_graph, category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path1 = 'video_frames/frame454.jpg'\n",
    "path2 = 'video_frames/frame459.jpg'\n",
    "path3 = 'video_frames/frame201.jpg'\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optical_flow(path1, path2):\n",
    "    print(path1)\n",
    "    print(path2)\n",
    "    frame1 = cv2.imread(path1)\n",
    "    frame2 = cv2.imread(path2)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[...,1] = 255\n",
    "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    nxt = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    #p0 = cv2.goodFeaturesToTrack(prvs, mask = None, **feature_params)\n",
    "    #p1, st, err = cv2.calcOpticalFlowPyrLK(prvs,nxt, p0, None, **lk_params)\n",
    "    #mask = np.zeros_like(frame1)\n",
    "    #print(np.shape(prvs))\n",
    "    #print(np.shape(nxt))\n",
    "    #flow = cv2.calcOpticalFlowFarneback(prvs,nxt,None, 0.5,3.0,15.0,3.0,5.0,1.2,0.0)\n",
    "    #flow = cv2.calcOpticalFlowFarneback(prvs,nxt,0.5,1,3,15,3,5,1,-1)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    #return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_frames/frame454.jpg\n",
      "video_frames/frame459.jpg\n",
      "(720, 1280)\n",
      "(720, 1280)\n"
     ]
    }
   ],
   "source": [
    "get_optical_flow(path1, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "person_to_boxes = defaultdict(list)\n",
    "for entry in stuff[0]:\n",
    "    if stuff[1][entry['person_id']] != \"unknown\":\n",
    "        person_to_boxes[entry['person_id']].append(entry['box'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in person_to_boxes:\n",
    "    label = stuff[1][i][0]\n",
    "    box = person_to_boxes[i][0]\n",
    "    center_x = (box[3]+box[1])/2.0\n",
    "    if center_x < 0.5 and label == \"l\":\n",
    "        correct+=1\n",
    "    if center_x >=0.5 and label == \"r\":\n",
    "        correct+=1\n",
    "print(correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
